{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM23Rlhw5aTcNZ3TVajkEDI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cpalme28/MAT-421/blob/main/Module_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Homework 4 - Coree Palmer\n",
        "\n",
        "**19.1 - Root Finding Problem Statement**\n",
        "\n",
        "The root or zero of a function, $f(x)$, is an $x_r$ such that $f(x_r) = 0$. For some functions, determining an analytic solution for the roots of functions can be difficult. In those cases, it is helpful to generate numerical approximations of the roots of f."
      ],
      "metadata": {
        "id": "hH736VA0PTya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the root of f(x) = cos(x) - x near -2\n",
        "\n",
        "import numpy as np\n",
        "from scipy import optimize\n",
        "\n",
        "f = lambda x: np.cos(x) - x\n",
        "r = optimize.fsolve(f, -2)\n",
        "print(\"r = \", r)\n",
        "\n",
        "# verification\n",
        "result = f(r)\n",
        "print(\"result = \", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnuB2unIQFBF",
        "outputId": "f6012787-74b6-4206-9568-78a8c125734f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "r =  [0.73908513]\n",
            "result =  [0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result of the function is 0 because 0.73908513 is a root, showing that fsolve works properly.\n",
        "\n",
        "The function f(x) = 1/x has no root, however. Using the fsolve function to compute it will showcase what will happen when there is no root."
      ],
      "metadata": {
        "id": "UTzGS-ShQiSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = lambda x: 1/x**3\n",
        "\n",
        "r, infodict, ier, mesg = optimize.fsolve(f, -2, full_output=True)\n",
        "print(\"r = \", r)\n",
        "\n",
        "result = f(r)\n",
        "print(\"result = \", result)\n",
        "\n",
        "print(mesg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5DK1KLUQ4IL",
        "outputId": "b2058a25-41d1-40d6-ac5b-9f6b1ba91d88"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "r =  [-6.3981792e+34]\n",
            "result =  [-3.81795496e-105]\n",
            "The number of calls to function has reached maxfev = 400.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the output shows, the value of r is not a root, despite f(r) being a very small number. Turning on full_output allows us to see more information, letting us understand more into the failure."
      ],
      "metadata": {
        "id": "yeogDtOORMHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**19.2 - Tolerance**\n",
        "\n",
        "Error is a deviation from an expected or computed value. Tolerance is the level of error that is acceptable for our usage. It is said that a computer program has converged to a solution when it has found a solution with an error smaller than the tolerance.\n",
        "\n",
        "For computing roots, we want an $x_r$ such that $f(x_r)$ is very close to 0. Therefore $|f(x)|$ is a possible choice for the measure of error since the smaller it is, the likelier we are to the root. If we assume that $x_i$ is the ith guess of an algorithm for finding a root, then $|x_{i-1} - x_i|$ is another possible choice for measuring error, since we expect the improvements between subsequent guesses to diminish as it approaches a solution.\n"
      ],
      "metadata": {
        "id": "hElb5ZugRlRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example:\n",
        "\n",
        "Let the error be measured by e = $|f(x)|$ and tol be the acceptable level of error. The function $f(x) = x^2 + tol/2$ has no real roots. However, $|f(0)| = tol/2$ and is therefore acceptable as a solution for root finding program."
      ],
      "metadata": {
        "id": "kl9m4O1NT_md"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**19.3 - Bisection Method**\n",
        "\n",
        "* The Intermediate Value Theorem (IVT) says that if f(x) is a continuous function between a and b, and the signs of f(a) and f(b) aren't the same, then there must be a c, such that a < c < b and f(c) = 0.\n",
        "\n",
        "The bisection method uses the IVT iteratively to find roots.\n",
        "* Let f(x) be a continuous function, and a and b be real scalar values such that a < b.\n",
        "* Assume, without loss of generality, that f(a) > 0 and f(b) < 0.\n",
        "* By the IVT, there must be a root on the open interval (a, b).\n",
        "* Let m = (b + a) / 2, the midpoint between a and b.\n",
        "* If f(m) = 0 or is close enough, then m is a root.\n",
        "* If f(m) > 0, then m is an improvement on the left bound, a, and there is guaranteed to be a root on the open interval (m, b).\n",
        "* If f(m) < 0, then m is an improvement on the right bound, b, and there is guaranteed to be a root on the open interval (a, m).\n",
        "* The process can be repeated until the error is acceptably low\n",
        "\n",
        "The square root of 2 can be computed as the root of the function $f(x) = x^2 - 2$. Starting at a = 0 and b = 2, we can use a function, my_bisection, to approximate the square root of 2 to a tolerance of $|f(x)| < 0.1$ and $|f(x) < 0.01$."
      ],
      "metadata": {
        "id": "gbXA5Y1NUCP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def my_bisection(f, a, b, tol):\n",
        "  # check to see if a and b bound a root\n",
        "  if np.sign(f(a)) == np.sign(f(b)):\n",
        "    raise Exception(\"The scalars a and b do not bound a root\")\n",
        "\n",
        "  # get a midpoint\n",
        "  m = (a + b) / 2\n",
        "\n",
        "  if np.abs(f(m)) < tol:\n",
        "    # check to see if root\n",
        "    return m\n",
        "  elif np.sign(f(a)) == np.sign(f(m)):\n",
        "    # case where m is an improvement on a\n",
        "    # let a = m\n",
        "    return my_bisection(f, m, b, tol)\n",
        "  elif np.sign(f(b)) == np.sign(f(m)):\n",
        "    # case where m is an improvement on b\n",
        "    # let b = m\n",
        "    return my_bisection(f, a, m, tol)\n",
        "\n",
        "f = lambda x: x**2 - 2\n",
        "\n",
        "r1 = my_bisection(f, 0, 2, 0.1)\n",
        "print(\"r1 = \", r1)\n",
        "r01 = my_bisection(f, 0, 2, 0.01)\n",
        "print(\"r01 = \", r01)\n",
        "\n",
        "print(\"f(r1) = \", f(r1))\n",
        "print(\"f(r01) = \", f(r01))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKaRriIEVTae",
        "outputId": "0fb0bbc2-712e-4e72-c28d-8a53a0229fd4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "r1 =  1.4375\n",
            "r01 =  1.4140625\n",
            "f(r1) =  0.06640625\n",
            "f(r01) =  -0.00042724609375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be observed, a result is returned sooner with a larger tolerance. The result with the smaller tolerance, r01, is a much more accurate result when compared to the root."
      ],
      "metadata": {
        "id": "W8cn6MLEX5zp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**19.4 - Newton-Raphson Method**\n",
        "\n",
        "Let f(x) be a smooth and continuous function and $x_r$ be an unknown root of f(x). Now assume that $x_0$ is a guess for $x_r$. Since $x_0$ is probably not the actual root, we are going to want to find an $x_1$ that is an improvement on $x_0$.\n",
        "\n",
        "The linear approximation of f(x) around $x_0$ is $f(x)$ is approximately equal to $f(x_0) + f'(x_0)(x - x_0)$.\n",
        "* Written generally, a Newton step computed an improved geuss, $x_i$, using a previous guess $x_{i-1}$, and is given by the following equation: $x_i = x_{i-1} - g(x_{i-1}) / g'(x_{i-1})$.\n",
        "* The Newton-Raphson Method of finding roots iterates Newton steps from $x_0$ until the error is less than the tolerance.\n",
        "\n",
        "Using the same example from the bisection method, we will compute the root using the Newton-Raphson Method."
      ],
      "metadata": {
        "id": "7YTZovXTYMUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "f = lambda x: x**2 - 2\n",
        "f_prime = lambda x: 2*x\n",
        "netwon_raphson = 1.4 - (f(1.4)) / (f_prime(1.4))\n",
        "\n",
        "print(\"newton_raphson = \", netwon_raphson)\n",
        "print(\"sqrt(2) = \", np.sqrt(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDZTt_n4aVGz",
        "outputId": "62d961da-1857-4e90-de8b-b0644ed28b54"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "newton_raphson =  1.4142857142857144\n",
            "sqrt(2) =  1.4142135623730951\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following is a function where the output is an estimation of the root f, f is a function object f(x), df is a function object to f'(x), x0 is an initial guess, and tol is the error tolerance. The error measurement should be |f(x)|."
      ],
      "metadata": {
        "id": "jggRUSrmcsEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_newton(f, df, x0, tol):\n",
        "  # uses newon raphson method\n",
        "  # recursive implementation\n",
        "  if abs(f(x0)) < tol:\n",
        "    return x0\n",
        "  else:\n",
        "    return my_newton(f, df, x0 - f(x0)/df(x0), tol)"
      ],
      "metadata": {
        "id": "8c19w98TamOn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now going to use it to compute square root of 2 to within tolerance 1e-6 starting at x0 = 1.5."
      ],
      "metadata": {
        "id": "DpsgSa1ddJg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "estimate = my_newton(f, f_prime, 1.5, 1e-6)\n",
        "print(\"estimate = \", estimate)\n",
        "print(\"sqrt(2) = \", np.sqrt(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IO8xIOHndIxc",
        "outputId": "cae5fb2c-36b5-42cd-d446-ea7b07b5b7ce"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "estimate =  1.4142135623746899\n",
            "sqrt(2) =  1.4142135623730951\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If $x_0$ is close to $x_r$, then it can be proven that, in general, the Newton-Raphson method converges to $x_r$ much faster than the bisection method. Since $x_r$ is unknown, though, there is no way to know if the initial guess is close enough to the root to get this behavior unless some special information about the function is known ahead of time. Another limitation comes when the derivative at a guess is close to 0. Then the Newton step will be very large and probably lead away from the root. Depending on the behavior of the function derivate between $x_0$ and $x_r$, the Newton-Raphson method may converge to a different root than $x_r$ that may not be useful for our intention."
      ],
      "metadata": {
        "id": "AruZ5gfxdbEZ"
      }
    }
  ]
}